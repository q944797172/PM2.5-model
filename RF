
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import math
from pandas.core.common import random_state
from scipy.stats import gaussian_kde
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt
import matplotlib
from sklearn.model_selection import train_test_split
import pickle
from sklearn.model_selection import KFold

def scatter_plot(TureValues,PredictValues):
    xxx = [-100.5,500.5]
    yyy = [-100.5,500.5]
    T_m=round(max(TureValues))
    P_m=round(max(PredictValues))
    xy = np.vstack([TureValues, PredictValues])
    z = gaussian_kde(xy)(xy)
    plt.figure()
    font1 = {'family': 'Times New Roman',
             'weight': 'bold',
             'size': 18,
             }

    plt.subplots_adjust(left=0.15, right=0.95, top=0.95, bottom=0.15)
    plt.tick_params(labelsize=16)
    plt.plot(xxx , yyy , c='0' , linewidth=1 , linestyle=':' , marker='.' , alpha=0.4)
    # plt.scatter(TureValues , PredictValues , s=20 , c='r' , edgecolors='k' , marker='o' , alpha=0.8,cmap='Spectral')
    plt.scatter(TureValues, PredictValues, s=20, c=z,cmap='Spectral') 
    plt.xlim((0,300))  
    plt.ylim((0,300))
    # plt.figure(figsize=(10, 8), dpi=100)
    plt.xlabel("Actual value (μg/m³)",font1)

    plt.ylabel("Predictive value (μg/m³)",font1)
    plt.title('')
    plt.show()


data=pd.read_excel('data.xlsx',index_col=0)

print (data.head())
print (data.shape)

index=data.index
col=data.columns
class_names=np.unique(data.iloc[:,1])
print (type(data))
print (class_names)
#print (data.describe())


data_train, data_test= train_test_split(data,test_size=0.2,random_state=0)

print ("train：\n",data_train.describe().round(2))
print ("test：\n",data_test.describe().round(2))
print ("train information：\n",data_train.iloc[:,-1].value_counts())
print ("test information：\n",data_test.iloc[:,-1].value_counts())

X_train=data_train.iloc[:,1:-1]
X_test=data_test.iloc[:,1:-1]
feature=data_train.iloc[:,1:-1].columns
print (feature)
y_train=data_train.iloc[:,-1]
y_test=data_test.iloc[:,-1]

from sklearn.model_selection import RandomizedSearchCV
criterion=['mse','mae']
n_estimators = [int(x) for x in np.linspace(start = 500, stop =3000, num = 500)]
max_features = ['auto', 'sqrt']
max_depth = [int(x) for x in np.linspace(20, 60, num = 10)]
max_depth.append(None)
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4]
bootstrap = [True, False]
random_grid = {'criterion':criterion,
                'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

clf= RandomForestRegressor()
clf_random = RandomizedSearchCV(estimator=clf, param_distributions=random_grid,
                              n_iter = 10,
                              cv = 3, verbose=2, random_state=42, n_jobs=-1)

clf_random.fit(X_train, y_train)
print (clf_random.best_params_)



from pyecharts import Bar
rf=RandomForestRegressor(criterion='mse',bootstrap=False,max_features='sqrt', max_depth=30,min_samples_split=10, n_estimators=2000,min_samples_leaf=2)

rf.fit(X_train, y_train)
y_train_pred=rf.predict(X_train)
y_test_pred=rf.predict(X_test)

from sklearn.metrics import mean_squared_error,explained_variance_score,mean_absolute_error,r2_score

scatter_plot(y_train,y_train_pred)

print ('R2:',rf.score(X_train,y_train))
print ('MSE',mean_squared_error(y_train,y_train_pred))
print ('MAE',mean_absolute_error(y_train,y_train_pred))

print ('R2:',rf.score(X_test,y_test))
print ('MSE',mean_squared_error(y_test,y_test_pred))
print ('MAE',mean_absolute_error(y_test,y_test_pred))

pickle.dump(rf,open("RF.dat","wb"))

