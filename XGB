import os
import xgboost as xgb
from xgboost import plot_importance
import pandas as pd
import numpy as np
from scipy.stats import gaussian_kde
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error,explained_variance_score,mean_absolute_error,r2_score

def scatter_plot(TureValues,PredictValues):

    xxx = [-100.5,500.5]
    yyy = [-100.5,500.5]
    print(len(TureValues))
    T_m=round(max(TureValues))
    P_m=round(max(PredictValues))
    xy = np.vstack([TureValues, PredictValues])
    z = gaussian_kde(xy)(xy)

    plt.figure()
    font1 = {'family': 'Times New Roman',
             'weight': 'bold',
             'size': 18,
             }

    plt.subplots_adjust(left=0.15, right=0.95, top=0.95, bottom=0.15)
    plt.tick_params(labelsize=16)
    plt.plot(xxx , yyy , c='0' , linewidth=1 , linestyle=':' , marker='.' , alpha=0.4)
    # plt.scatter(TureValues , PredictValues , s=20 , c='r' , edgecolors='k' , marker='o' , alpha=0.8,cmap='Spectral')
    plt.scatter(TureValues, PredictValues, s=20, c=z,cmap='Spectral')  
    plt.xlim((0,300))   
    plt.ylim((0,300))
    # plt.figure(figsize=(10, 8), dpi=100)
    plt.xlabel("Actual value (μg/m³)",font1)

    plt.ylabel("Predictive value (μg/m³)",font1)
    plt.title('')
    plt.show()

os.listdir(os.getcwd())

data=pd.read_excel('Data.xlsx')
print(data)
data.head()
data.rename(columns={"PM25":'label'},inplace=True)

data.shape
data.columns
from sklearn import preprocessing
print(data)
from sklearn.model_selection import train_test_split

# x_train,x_test,y_train,y_test=train_test_split(data.drop(['label'],axis=1),data['label'],test_size=0.2,random_state=0)
# print(x_test)
data_train, data_test= train_test_split(data,test_size=0.15,random_state=0)
x_train=data_train.iloc[:,0:-1]
x_test=data_test.iloc[:,0:-1]
feature=data_train.iloc[:,0:-1].columns
print (feature)
y_train=data_train.iloc[:,-1]
y_test=data_test.iloc[:,-1]

feature=data_train.iloc[:,1:-1].columns
xgb_train=xgb.DMatrix(x_train,label=y_train)
xgb_test=xgb.DMatrix(x_test,label=y_test)
print(xgb_train)

params={
    'booster':'gbtree',
    'objective':'reg:linear',
    'gamma':0.15,
    'max_depth':20,
    'lambda':10,
    'subsample':0.8,
    'min_child_weight': 3,
    'eta':0.01
}
num_rounds=3000
watchlist=[(xgb_train,'train'),(xgb_test,'test')]
model=xgb.train(params,xgb_train,num_rounds,watchlist)

model.save_model('xgb.model')
model=xgb.Booster(model_file='xgb.model')
y_train_pred=model.predict(xgb.DMatrix(x_train))
y_predicted=model.predict(xgb.DMatrix(x_test))

print ('MSE',mean_squared_error(y_train,y_train_pred))
print ('MAE',mean_absolute_error(y_train,y_train_pred))
print ('R2',explained_variance_score(y_train,y_train_pred))

print ('MSE',mean_squared_error(y_test,y_predicted))
print ('MAE',mean_absolute_error(y_test,y_predicted))
print ('R2',explained_variance_score(y_test,y_predicted))
print(y_predicted)
scatter_plot(y_train,y_train_pred)
scatter_plot(y_test,y_predicted)
plot_importance(model)
plt.show()
importance_dict = model.get_fscore()
total_importance = sum(importance_dict.values())
normalized_importance_dict = {feat: importance / total_importance for feat, importance in importance_dict.items()}
sorted_importance_dict = {feat: normalized_importance_dict[feat] for feat in sorted(normalized_importance_dict, key=normalized_importance_dict.get, reverse=True)}
feature_names = list(sorted_importance_dict.keys())
normalized_importance = list(sorted_importance_dict.values())
